{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3213391d",
   "metadata": {},
   "source": [
    "\n",
    "# 🚩 Example 1: Simple ML Pipeline Example: Predict House Prices  \n",
    "\n",
    "Simple example of a basic ML pipeline for **supervised learning** using `scikit-learn` with a linear regression model:   \n",
    "\n",
    "### ✅ A very compact supervised ML pipeline in Python.\n",
    "- **Get Data & Libraries**\n",
    "- **Train-Test Split**\n",
    "- **Model Training (Linear Regression)**\n",
    "- **Prediction**\n",
    "- **Performance Metric (MSE)**  \n",
    "\n",
    "### 📊 Dataset & Libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e364178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (house size in sqft vs. price in $1000)\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = {'Size': [2100, 1600, 2400, 1416],\n",
    "        'Price': [399.9, 329.9, 369.0, 232.0]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55931183",
   "metadata": {},
   "source": [
    "### Step 1: Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ed61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Size']]   # features\n",
    "y = df['Price']    # target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59d4b9",
   "metadata": {},
   "source": [
    "### Step 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4435b237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ced6b",
   "metadata": {},
   "source": [
    "### Step 3: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e7cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bcee0a",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fb1757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3009.7609532103047\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b7b17",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82933cdf",
   "metadata": {},
   "source": [
    "# 🚩 Example 2: Here's another example of a machine learning pipeline for supervised learning using \n",
    "- the Iris dataset and \n",
    "- scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63244a8",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff28cc",
   "metadata": {},
   "source": [
    "### 1. Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d648604",
   "metadata": {},
   "source": [
    "### 2. Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb602a43",
   "metadata": {},
   "source": [
    "### 3. Create ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ef2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pipeline first scales the data, then applies a classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Data preprocessing step\n",
    "    ('classifier', RandomForestClassifier())  # Model training step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11df4dd",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509834b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff23051",
   "metadata": {},
   "source": [
    "### 5. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd785e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd1e04",
   "metadata": {},
   "source": [
    "### 6. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b398eb9",
   "metadata": {},
   "source": [
    "## Key Steps in the Pipeline:\n",
    "\n",
    "1. **Data Loading**: Load the dataset (Iris in this case)\n",
    "2. **Data Splitting**: Divide into training and test sets\n",
    "3. **Pipeline Creation**: \n",
    "   - Preprocessing: Standardize features (mean=0, variance=1)\n",
    "   - Model: Random Forest classifier\n",
    "4. **Model Training**: Fit the pipeline on training data\n",
    "5. **Prediction**: Make predictions on test data\n",
    "6. **Evaluation**: Assess model performance. Compute accuracy score.\n",
    "\n",
    "This is a minimal example - real-world pipelines often include more steps like feature engineering, hyperparameter tuning, and cross-validation.\n",
    "\n",
    "> 📌 *In real-world cases: add feature engineering, hyperparameter tuning, cross-validation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f41b8b",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **What is a ML/DL Pipeline?**\n",
    "A **Machine Learning (ML) or Deep Learning (DL) pipeline** is an automated sequence of steps that takes raw data as input and transforms it into a trained model ready for predictions. It typically includes:\n",
    "\n",
    "- **Data Preprocessing** (cleaning, normalization, feature engineering)  \n",
    "- **Model Training** (selecting & training an algorithm)  \n",
    "- **Evaluation** (testing model performance)  \n",
    "- **Deployment** (making the model available for predictions)  \n",
    "\n",
    "In DL pipelines, additional steps like **_neural network architecture_** design and GPU acceleration are often included.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Why is it Called a \"Pipeline\"?**\n",
    "The term comes from **industrial pipelines** where materials flow through connected stages to be processed. Similarly, in ML:\n",
    "\n",
    "- Data \"flows\" through sequential stages  \n",
    "- Each step transforms the data/model further  \n",
    "- The output of one step becomes the input of the next  \n",
    "\n",
    "**Example:**  \n",
    "`Raw Data -> Cleaned Data -> Scaled Data -> Trained Model -> Predictions`\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Why is it Important?**\n",
    "ML pipelines are critical because they:\n",
    "\n",
    "1. **Standardize Workflow**  \n",
    "   - Ensures consistency (every experiment follows the same steps).  \n",
    "   - Avoids errors (e.g., forgetting to scale data before training).  \n",
    "\n",
    "2. **Enable Automation**  \n",
    "   - Automatically reprocess data when new samples arrive.  \n",
    "   - Facilitate hyperparameter tuning and retraining.  \n",
    "\n",
    "3. **Improve Reproducibility**  \n",
    "   - Makes it easier to share/replicate results.  \n",
    "\n",
    "4. **Simplify Deployment**  \n",
    "   - Packaging preprocessing + model into a single pipeline avoids \"training-serving skew\".  \n",
    "\n",
    "5. **Save Time**  \n",
    "   - Avoids manually re-running each step during iterations.  \n",
    "\n",
    "\n",
    "\n",
    "### 🚩 Example from Previous Code (Example 2):\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),       # Step 1: Preprocess\n",
    "    ('classifier', RandomForestClassifier())  # Step 2: Train\n",
    "])\n",
    "```\n",
    "Here, `StandardScaler` and `RandomForestClassifier` are \"piped\" together—data flows through them sequentially.  \n",
    "\n",
    "\n",
    "\n",
    "### 📌 Pipeline Structure:\n",
    "```python\n",
    "Pipeline([\n",
    "    ('step1', Transformer1()),\n",
    "    ('step2', Transformer2()),\n",
    "    ...\n",
    "    ('final_model', Estimator())\n",
    "])\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "### **`sklearn.pipeline.Pipeline` Explained**\n",
    "\n",
    "The `Pipeline` class from `sklearn.pipeline` is a **scikit-learn tool that chains multiple data processing and modeling steps into a single object**. It ensures that all steps are executed in sequence, making ML workflows more efficient, organized, and less error-prone.\n",
    "\n",
    "\n",
    "**Benefits:**\n",
    "- Sequentially applies transformers and an estimator.\n",
    "- Ensures correct processing order.\n",
    "- Simplifies `.fit()`, `.predict()`, `.score()`.\n",
    "- Prevents data leakage.\n",
    "- Supports hyperparameter tuning with `GridSearchCV`.\n",
    "\n",
    "---\n",
    "\n",
    "## **What Does It Do?**\n",
    "1. **Sequential Execution**  \n",
    "   - Applies a series of **transformers** (preprocessing steps) followed by a final **estimator** (ML model).  \n",
    "   - Example:  \n",
    "     ```python\n",
    "     Pipeline([\n",
    "         ('scaler', StandardScaler()),       # Step 1: Preprocess data\n",
    "         ('model', LogisticRegression())     # Step 2: Train model\n",
    "     ])\n",
    "     ```\n",
    "     Here, data first goes through `StandardScaler()` before being passed to `LogisticRegression()`.\n",
    "\n",
    "2. **Ensures Correct Order**  \n",
    "   - Automatically applies steps in the defined sequence.  \n",
    "   - Prevents mistakes like **fitting the scaler on test data** or **forgetting to transform features before prediction**.\n",
    "\n",
    "3. **Single Interface for Fit/Predict**  \n",
    "   - You can call `.fit()`, `.predict()`, or `.score()` on the entire pipeline, and scikit-learn handles the intermediate steps.  \n",
    "   - Example:\n",
    "     ```python\n",
    "     pipeline.fit(X_train, y_train)  # Applies scaler.fit_transform() then model.fit()\n",
    "     y_pred = pipeline.predict(X_test)  # Applies scaler.transform() then model.predict()\n",
    "     ```\n",
    "\n",
    "4. **Avoids Data Leakage**  \n",
    "   - Prevents test data from influencing preprocessing (e.g., scaler learns only from training data).  \n",
    "   - Critical for reliable model evaluation.\n",
    "\n",
    "5. **Simplifies Hyperparameter Tuning**  \n",
    "   - Works seamlessly with `GridSearchCV` or `RandomizedSearchCV`.  \n",
    "   - Example: Tune both scaler and model parameters in one go:\n",
    "     ```python\n",
    "     params = {\n",
    "         'scaler__with_mean': [True, False],  # Parameters for StandardScaler\n",
    "         'model__C': [0.1, 1, 10]            # Parameters for LogisticRegression\n",
    "     }\n",
    "     grid_search = GridSearchCV(pipeline, params)\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use `Pipeline`? (Key Benefits)**\n",
    "✅ **Cleaner Code** – No need to manually apply each step.  \n",
    "✅ **Prevents Bugs** – Eliminates mistakes in data flow (e.g., forgetting to scale test data).  \n",
    "✅ **Reproducibility** – Encapsulates the entire workflow in one object.  \n",
    "✅ **Easy Deployment** – Deploy a single pipeline (preprocessing + model) instead of separate steps.  \n",
    "✅ **Hyperparameter Tuning** – Optimize all steps together in `GridSearchCV`.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example 3:** Example Without vs. With Pipeline\n",
    "#### ❌ **Without Pipeline (Manual Steps)**\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit scaler on training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Scale test data (must remember to do this!)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "```\n",
    "⚠️ **Problems:**  \n",
    "- Manual steps increase risk of errors (e.g., forgetting `scaler.transform`).  \n",
    "- Harder to maintain and deploy.  \n",
    "\n",
    "\n",
    "#### ✅ **With Pipeline (Automated)**\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Just fit and predict—everything handled automatically!\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "✔️ **Advantages:**  \n",
    "- No manual intermediate steps.  \n",
    "- Prevents data leakage.  \n",
    "- Easier to maintain and deploy.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 📌 Summary:\n",
    "| 📌 With Pipeline | ❌ Without Pipeline |\n",
    "|:----------------|:------------------|\n",
    "| Cleaner, organized code | Risk of manual errors |\n",
    "| Automatic data flow | Hard to track workflow |\n",
    "| Prevents data leakage | Possible preprocessing mistakes |\n",
    "| Supports hyperparameter tuning | More complicated to tune |\n",
    "| Easier deployment | Tedious to manage separately |\n",
    "\n",
    "\n",
    "\n",
    "### **When Should You Use `Pipeline`?**\n",
    "- **Always** (unless working with trivial models).  \n",
    "- Especially useful when:  \n",
    "  - You have multiple preprocessing steps (e.g., scaling, PCA, feature selection).  \n",
    "  - You want to avoid data leakage.  \n",
    "  - You need hyperparameter tuning across steps.  \n",
    "  - You plan to deploy the model (saving one pipeline is easier than managing multiple steps).  \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
