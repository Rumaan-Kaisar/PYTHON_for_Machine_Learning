
################# 0: FULL
# copy: Analyze: analyze this text, get the topics, 
#       output in english: describe the topics on your own knowledge withot losing context, use clarification and fix any misconception (mention them)
#        
################# (10-May-25 for 11-May-25)

# Courses: PrTla PY for DS & ML >    14.4, 14.5


----------------------    Use google translate:    -----------------------------

------------------------------
00:03:55,430 --> 00:03:56,910
de prueba x.
------------------------------


==================  9.00  =====================
Confusion matrix:

Now we can also see all the "correctly classified" versus "incorrectly classified" images in the form of a confusion matrix.
So the confusion matrix looks something like this:

multi level Table:
----------------------------------------------------------------------------------------------------------
|                    |                      | **Predicted Condition**        |                                 |
| :----------------- | :------------------- | :----------------------------- | :------------------------------ |
|                    | **Total Population** | **Predicted Positive**         | **Predicted Negative**          |
| **True Condition** | **Actual Positive**  | True Positive (TP)             | False Negative (FN) *(Type II error)* |
|                    | **Actual Negative**  | False Positive (FP) *(Type I error)* | True Negative (TN)              |
----------------------------------------------------------------------------------------------------------

true condition:
We have the underlying true conditions.
That's the true correct label, and we can think of the condition as positive or negative, like
actually being a dog or not being a dog, or often used in a medical diagnosis, like having the presence of
a disease as opposed to not having it.


predicted condition
We have the predicted condition from the models.
So, positive prediction or negative prediction, and I think when it comes to confusion matrices, it's often
easier to first get an intuition if you think of this as a diagnostic test for having some type of disease present
in a person after maybe taking a blood sample and running it through your model.

---------------  GPT  ---------------


Sure ‚Äî here‚Äôs a simplified, clear version without losing context:

---

### üìä **Confusion Matrix Simplified**

* A **confusion matrix** shows how many predictions were **correct** or **incorrect** by comparing predicted and actual labels.

---

**Structure:**

|                     | **Predicted Positive**               | **Predicted Negative**                |
| :-----------------: | :----------------------------------- | :------------------------------------ |
| **Actual Positive** | True Positive (TP)                   | False Negative (FN) *(Type II error)* |
| **Actual Negative** | False Positive (FP) *(Type I error)* | True Negative (TN)                    |

---

**Key Ideas:**

* **True Condition:**
  The actual, correct label ‚Äî e.g. whether the image is *actually a dog* or *not a dog*, or in medical tests, *has a disease* or *doesn't have it*.

* **Predicted Condition:**
  What the model predicts ‚Äî positive or negative.

* **Use Case Example:**
  Think of this like a medical test:

  * *Positive prediction* ‚Üí model says person has disease
  * *Negative prediction* ‚Üí model says person doesn‚Äôt
    The confusion matrix tracks where the model was right and wrong.

---

Would you like a small diagram-style illustration version too?



|                    |                      | **Predicted Condition**        |                                 |
| :----------------- | :------------------- | :----------------------------- | :------------------------------ |
|                    | **Total Population** | **Predicted Positive**         | **Predicted Negative**          |
| **True Condition** | **Actual Positive**  | True Positive (TP)             | False Negative (FN) *(Type II error)* |
|                    | **Actual Negative**  | False Positive (FP) *(Type I error)* | True Negative (TN)              |


----  9.48  ----


So, for example, a true positive would be someone who actually has that disease and your model correctly predicts

that they have it; a true negative would be someone who doesn't have the disease and your model correctly predicts

that they don't have the disease.

So, you have true positives and true negatives, and both are correct predictions, and then we essentially have

these two types of incorrect predictions: a false positive and a false negative. A false positive would be if the person

doesn't have the disease, and you predict that it's a false positive because you're falsely saying that they're

positive for this disease or this condition. A false negative is essentially

the opposite of where this person has the disease present, and then you report using your model and your

test that they're actually negative, they actually don't have this disease, and sometimes

they're also called, and the statistics are, a type 1 error and a type 2 error, and

then from these true negative results, true positive results, false negative results, and false positive results.

There are many other metrics you can calculate, so we can see some of them here.

In the bottom left corner, we can see the actual accuracy calculation. It's the sum of true positives plus true negatives, essentially the sum of the total population guesses.

There are a bunch of others, like the positive probability rate, false positive rate, prevalence rate, and false positive rate.




















Part 2: 

Welcome back, everyone.

We just discussed how to evaluate performance for classification problems.

Now let's understand how to evaluate performance for regression testing, so we can take a moment to discuss how we evaluate a model that is performing some type of regression task.

And, in general, a regression is the task when a model tries to predict continuous values.

Unlike classification tasks, where we are trying to predict categorical values, you may have heard of some evaluation metrics, such as precision or recall, as we just discussed.

However, these types of metrics will not be useful for regression problems.

It doesn't really make sense to calculate the precision or recall of a regression task since you are actually classifying things.

Instead, you are predicting a continuous value.

Therefore, we need new metrics designed for those continuous values.

For example, imagine we are trying to predict the price of a house given its characteristics.

That would be a regression task, or trying to predict the country.

A house is inside.

Given its characteristics, it would be a classification task.

And again, we're focused now on how to evaluate that regression task where a label is continuous and not separate categories.

So, again, to look at the most common evaluation metrics for regression, which are the

mean absolute error (mean squared error) and then the root mean squared error.

Let's start with the simplest, which is the mean absolute error.

And this is the easiest one to understand. Essentially, all you're going to do is compare your predictions.

And remember, we're comparing a continuous value here since this is a regression task.

We're going to compare our predictions with the label and the true one. For example, compare the actual house price prediction with the actual house price.

And what we do is simply take the difference between the true price minus our predicted price. That's why we take the absolute value of that. The reason we take the absolute value is because technically, our predictions could be above or below the true value.

And since we want to average all of our predictions, we take the absolute value.

Okay, that's the mean absolute error. You're essentially just taking the differences between your prediction and the true label. You take the absolute value of that, and then you average that for all of your predictions.

Now, the problem with the mean absolute error is that it won't punish large errors.

So here we have what's known as an ans comes quartet, and we can see here that we have a wide variety

of different spreads of data points here.

However, the line of best fit for all of these is actually the same.

So we want to make sure we're aware of situations like this.

For example, let's take a look at this specific situation.

We have one point that's a huge outlier, so we want our air metrics to really account for them.

So what we can do is use a mean squared error.

And this is the mean of the squared errors.

So what we're doing here is taking the difference between the true value minus our predicted value and we're going to square it.

And as you can imagine, when we actually square that error, the larger errors are more noticeable than with mean absolute error, which makes squared error more popular because it's actually going to punish your model for those outliers it doesn't fit.

And we no longer need to take the absolute value because anything squared ends up being positive.

However, there's another problem we run into with a squared error that squares the actual label minus its prediction.

It actually also squares the units themselves.

So, for example, if you're predicting the price of a house, our error metric with mean absolute error would be in dollars, but with mean squared error.

We end up getting an error metric in units of dollars squared, which is hard to interpret, so the way we solve this is with the root mean squared error. Essentially, at the end, you just take the square root of your mean squared error.

So this is the most popular because it punishes those larger error values ‚Äã‚Äãand

at the end of the day, it has the same metrics or the same units as y now.

The most common question I get from students is: Hey, does this squared error mean this value?

I got it right.

And as always, context is everything.

Let's imagine you were doing some kind of machinery model and got a squared error.

























Bienvenidos a todos a esta conferencia sobre evaluaci√≥n del desempe√±o, particularmente para problemas de clasificaci√≥n en la

pr√≥xima conferencia.

Discutiremos la evaluaci√≥n del rendimiento para las protestas de regresi√≥n, por lo que acabamos de enterarnos de que despu√©s de que

se complete un proceso de aprendizaje autom√°tico, utilizaremos m√©tricas de rendimiento para evaluar c√≥mo lo hizo realmente

nuestro modelo.

Y seguimos mencionando el hecho de que entrenamos nuestro modelo en los datos de entrenamiento y luego usaremos

alg√∫n tipo de m√©trica para ver realmente c√≥mo se desempe√±√≥ en el conjunto de prueba o el conjunto de validaci√≥n.

Entonces sigamos y discutamos lo que eso realmente significa.

Qu√© m√©tricas de clasificaci√≥n vamos a utilizar y las m√©tricas de clasificaci√≥n clave

que deber√≠amos comprender nuestra precisi√≥n, precisi√≥n de recuerdo y puntaje f1, pero primero debemos entender el razonamiento detr√°s de estas

m√©tricas y c√≥mo funcionar√°n realmente en el mundo real t√≠picamente en cualquier tarea de clasificaci√≥n su modelo en

realidad solo puede lograr dos resultados, o bien su modelo fue correcto en su

predicci√≥n o su modelo fue incorrecto en su predicci√≥n y todas las m√©tricas de clasificaci√≥n provienen de esta idea, ahora

afortunadamente incorrecta versus correcta tambi√©n se expande a situaciones en las que tiene m√∫ltiples clases como

intentar para predecir categor√≠as de m√°s de dos.

Por ejemplo, tiene Categor√≠a A B C D Puede ser correcto al predecir la categor√≠a correcta o

incorrecto al predecir la categor√≠a correcta.

Ahora con el fin de explicar las m√©tricas, vamos a seguir adelante y simplificar esto a lo

que se conoce como una situaci√≥n de clasificaci√≥n binaria binaria que significa dos.

Por lo tanto, solo hay dos clases disponibles, ya sea clase 0 o clase 1.

Y esta idea tambi√©n se expandir√° a m√∫ltiples clases.

Pero para simplificar, imaginemos solo una situaci√≥n de clasificaci√≥n binaria, por lo que en nuestro ejemplo intentaremos predecir si una imagen

es un perro o un gato realmente realizar√° esta tarea m√°s adelante durante la convoluci√≥n de toda

la parte de la red neuronal de este curso.

Ahora, dado que es un problema de aprendizaje supervisado, lo que tendremos que hacer primero es ajustar o entrenar un modelo

sobre datos de entrenamiento.

Eso significa que tendremos im√°genes de que alguien ya se ha adelantado y etiquetado como perro o gato.

Entonces sabemos la respuesta correcta en estas im√°genes y luego vamos a probar ese modelo en los datos de prueba.

As√≠ que vamos a mostrar nuevas im√°genes que la modelo no ha visto antes.

Obtenga la predicci√≥n de los modelos y luego compare los resultados de la predicci√≥n del modelo con la

respuesta correcta que ya conocemos.

Entonces, una vez que tenemos las predicciones de los modelos de los datos de la prueba X, la comparamos con los valores verdaderos de y.

Las etiquetas correctas, as√≠ que en realidad diagramamos este proceso.

Imaginemos que ya hemos entrenado nuestro modelo en algunos datos de entrenamiento y ahora es el momento de evaluar

realmente el rendimiento del modelo.

Aqu√≠ es donde entra nuestro conjunto de datos de prueba.

Entonces tomamos una imagen de prueba de donde vamos a etiquetar x prueba X que significa caracter√≠stica.

Por lo tanto, la imagen en s√≠ misma es una caracter√≠stica y esto es del conjunto de prueba y hay una etiqueta

correcta correspondiente de la prueba Y.

Entonces tenemos la caracter√≠stica o imagen X y esa es la imagen de prueba y tambi√©n tenemos la etiqueta correcta de la

prueba Y.

Y en este caso podemos ver que esta imagen es una imagen de un perro.

Entonces, lo que vamos a hacer es alimentar las caracter√≠sticas.

En este caso, la imagen en nuestro modelo ya entrenado y luego el modelo har√° alguna predicci√≥n.

Entonces, el modelo predice que se trata de un perro y luego lo que hacemos es comparar la predicci√≥n con la

etiqueta correcta.

Entonces este perro igual al perro en este caso era correcto, sin embargo, tal vez predijo que esta imagen era un gato.

Y en este caso, esta comparaci√≥n con la etiqueta correcta ser√≠a incorrecta.

Y estas son esencialmente las dos √∫nicas situaciones.

O tienes raz√≥n o est√°s equivocado, correcto o incorrecto, as√≠ que repetimos este proceso para todas las im√°genes en nuestros datos



------------------------------
00:03:55,430 --> 00:03:56,910 :: DONE 9-May-2025
de prueba x.
------------------------------

de prueba x.

Y al final tendremos un recuento de coincidencias correctas y una cuenta de coincidencias incorrectas.

Y la realizaci√≥n clave aqu√≠ que tenemos que hacer es que en el mundo real no todas las coincidencias

incorrectas o correctas tienen el mismo valor.

Entonces, vamos a concentrarnos en lo que queremos decir con eso, de modo que en el mundo real una sola

m√©trica no contar√° la historia completa y, para comprender todo esto, recuperemos esas cuatro m√©tricas que mencionamos y veamos c√≥mo

se calculan realmente.

Podr√≠amos organizar nuestros valores pronosticados en comparaci√≥n con los valores reales y lo que se

conoce como una matriz de confusi√≥n, por lo que tocaremos la matriz de confusi√≥n m√°s adelante.

Pero primero hablemos de la precisi√≥n, la precisi√≥n es una de las m√©tricas de clasificaci√≥n m√°s comunes.

Y afortunadamente tambi√©n es el m√°s f√°cil de entender.

Es realmente intuitivo lo que mide la precisi√≥n de la precisi√≥n y los problemas

de clasificaci√≥n es la cantidad de predicciones correctas realizadas por el modelo dividido por la cantidad o la cantidad total de predicciones.

Entonces, de nuevo, es el n√∫mero de predicciones correctas dividido por el n√∫mero

total de predicciones esencialmente respondiendo a las preguntas cu√°ntas predicciones acerta como porcentaje, as√≠ que, por ejemplo, imaginemos

que el conjunto de prueba X ten√≠a 100 im√°genes y nuestro modelo predijo correctamente 80 im√°genes.

Luego tenemos 80 dividido por 100 o cero punto, ocho u 80 por ciento de precisi√≥n y la precisi√≥n es realmente √∫til cuando

las clases objetivo est√°n bien equilibradas.

Entonces que significa eso.

Bien equilibrado bien en nuestro ejemplo.

Eso significar√≠a que tenemos aproximadamente la misma cantidad de im√°genes de gatos que im√°genes de perros en todos nuestros datos.

Por lo tanto, significa que las etiquetas en s√≠ mismas est√°n m√°s o menos igualmente representadas en el conjunto de datos, pero imaginemos que tenemos

lo que se conoce como una situaci√≥n de clase desequilibrada.

En este caso, la precisi√≥n no es una buena m√©trica para usar.

As√≠ que hagamos un peque√±o experimento mental.

Imaginemos que en este conjunto de pruebas ten√≠amos noventa y nueve im√°genes de perros y solo una imagen de un gato.

Ahora, si nuestro modelo fuera simplemente una l√≠nea que siempre predijo el perro, sin importar la imagen que viera, obtendr√≠amos una precisi√≥n del

noventa y nueve por ciento en este conjunto de prueba en particular porque simplemente diciendo que el

perro noventa y nueve im√°genes son perros y una imagen de un gato entonces solo nos perdimos uno.

Es por eso que debes ser consciente de la desventaja de la precisi√≥n y la desventaja realmente viene

cuando tienes una situaci√≥n de clase desequilibrada.

Si sus clases est√°n m√°s o menos equilibradas, la precisi√≥n es un m√©todo intuitivo muy agradable o una m√©trica para comprender.

Sin embargo, si tiene una clase desequilibrada, puede ver en esta situaci√≥n particular

donde comienza a representar un problema y ah√≠ es donde entran en juego las otras m√©tricas.

Entonces, nuevamente en esta situaci√≥n, querremos comprender el recuerdo y la precisi√≥n que nos ayudan a comprender qu√© tan

bien se est√° desempe√±ando en el retiro de clases desequilibradas es la capacidad de un modelo para encontrar todos los casos relevantes dentro de

un conjunto de datos y la definici√≥n precisa del recuerdo es la cantidad conocidos como verdaderos positivos y lo vamos a

afinar m√°s adelante cuando veamos la matriz de confusi√≥n.

Pero es el n√∫mero de verdaderos positivos dividido por el n√∫mero de verdaderos positivos

m√°s el n√∫mero de falsos negativos y la precisi√≥n es la capacidad de un modelo de clasificaci√≥n para identificar solo los puntos

de datos relevantes donde la precisi√≥n se define como el n√∫mero de verdaderos positivos divididos por el

n√∫mero de positivos verdaderos m√°s un n√∫mero de falsos positivos ahora.

A menudo, usted tiene una compensaci√≥n entre el recuerdo y la precisi√≥n, mientras que el recuerdo expresa la capacidad de

encontrar todas las instancias relevantes en un conjunto de datos. La precisi√≥n expresa la proporci√≥n de los puntos

de datos que nuestro modelo dice que eran relevantes y que realmente eran relevantes, y luego el puntaje F1 es esencialmente una combinaci√≥n de

estos dos.


-----------------------------
00:07:42,670 --> 00:07:43,570 :: DONE 9-May-2025
estos dos.
-----------------------------

En los casos en que queremos encontrar una combinaci√≥n √≥ptima de precisi√≥n y recuperaci√≥n, podemos combinar

las dos m√©tricas utilizando lo que se conoce como la puntuaci√≥n F1 y la puntuaci√≥n F1 es la media arm√≥nica de precisi√≥n y recuperaci√≥n teniendo

en cuenta ambas m√©tricas en la siguiente ecuaci√≥n.

Entonces, esto no es solo tomar el promedio de precisi√≥n de registro, sino tomar la media arm√≥nica de ellos.

Y aqu√≠ tenemos la f√≥rmula para el puntaje F1 o los puntajes F1 iguales a dos veces.

Los tiempos de precisi√≥n se recuerdan en el numerador y luego se dividen por la precisi√≥n m√°s el recuerdo en el denominador, por lo

que la raz√≥n por la que usamos la media arm√≥nica en lugar de solo una media simple o un promedio

simple es porque va a castigar valores extremos.

Por ejemplo, si cre√≥ un clasificador que ten√≠a una posici√≥n de 1 que significaba una precisi√≥n esencialmente perfecta

y una recuperaci√≥n de cero era esencialmente el peor registro posible.

Si solo tomaras el promedio simple de esto, entonces ser√≠a cero punto cinco.

Pero el puntaje F1 porque se ha tomado un arm√≥nico significa que tenemos tiempos de precisi√≥n recuperados all√≠ en la parte superior, lo

que significa que obtienes cero veces uno de los mejores y obtienes un puntaje F1 de cero.

Esto es realmente bueno porque te permite castigar las diferencias extremas entre la precisi√≥n y la recuperaci√≥n, por lo

que te da una especie de evaluaci√≥n justa de esa compensaci√≥n entre precisi√≥n y recuperaci√≥n.

Ahora tambi√©n podemos ver todas las im√°genes clasificadas correctas versus clasificadas incorrectamente en forma de una

matriz de confusi√≥n.

Entonces, la matriz confusa se parece a esto.

Tenemos las verdaderas condiciones subyacentes.

Esa es la verdadera etiqueta correcta y podemos pensar que la condici√≥n es positiva o negativa, como

ser realmente un perro o no ser un perro o, a menudo, se usa en un diagn√≥stico m√©dico, como tener presencia de

una enfermedad en lugar de no tenerla. Tenemos la condici√≥n predicha de los modelos.

Entonces, predicci√≥n positiva o predicci√≥n negativa y creo que cuando se trata de matrices de confusi√≥n, a menudo es

m√°s f√°cil primero tener una intuici√≥n si piensas en esto como una prueba de diagn√≥stico para tener alg√∫n tipo de enfermedad presente

en una persona despu√©s de que tal vez tomes una muestra de sangre y ejec√∫talo a trav√©s de tu modelo.

Entonces, por ejemplo, un verdadero positivo ser√≠a alguien que realmente tiene esa enfermedad y su modelo que predice correctamente

que lo tiene, un verdadero negativo ser√≠a alguien que no tiene la enfermedad y su modelo

que predice correctamente que no tiene la enfermedad.

Entonces, tenga verdaderos positivos y verdaderos negativos, y ambas son predicciones correctas y luego tenemos esencialmente

estos dos tipos de predicciones incorrectas: un falso positivo y un falso negativo un falso positivo ser√≠an si la persona

no tiene la enfermedad y usted predice que es falso positivo porque est√°s diciendo falsamente que son

positivos para esta enfermedad o esta condici√≥n, un falso negativo es esencialmente

lo contrario de donde esta persona tiene la enfermedad presente y luego informas usando tu modelo y tu

prueba que en realidad es negativo, en realidad no tienen esta enfermedad y a veces

tambi√©n se les llama y las estad√≠sticas son un error de tipo 1 y un error de tipo 2 y

luego de estos resultados negativos verdaderos positivos verdaderos falsos negativos y resultados falsos positivos.

Hay muchas otras m√©tricas que puede calcular, por lo que podemos ver algunas de ellas aqu√≠,

como en la esquina inferior izquierda, podemos ver el c√°lculo real de la precisi√≥n, es la

suma de los verdaderos positivos m√°s los verdaderos negativos, esencialmente la suma de lo que acert√≥ la poblaci√≥n

total y hay un mont√≥n de otros como tasa de probabilidad positiva tasa de falsos positivos tasa de prevalencia de

falsos positivos tasa de



-----------------------------
00:11:08,300 --> 00:11:09,660
falsos positivos tasa de
-----------------------------


falsos descubrimientos, etc. Ahora el punto principal a recordar aqu√≠ es que la matriz de

confusi√≥n y las diversas m√©tricas calculadas son esencialmente todas formas fundamentales de comparar el valor predicho frente a los

valores verdaderos y lo que constituye una buena m√©trica en realidad depende de la situaci√≥n espec√≠fica.

Una pregunta muy com√∫n que recibo de los estudiantes es, oye, es una precisi√≥n lo suficientemente buena.

Bueno, eso realmente depende de su situaci√≥n y el contexto de la situaci√≥n.

Ahora, si todav√≠a est√°s confundiendo la matriz de confusi√≥n, no hay problema.

Echa un vistazo a la p√°gina de wikipedia para ello.

Tiene un diagrama realmente bueno con todo lo m√°s importante para todas las m√©tricas y, a lo largo

de la capacitaci√≥n, este curso generalmente solo iba a imprimir m√©tricas, como imprimir la precisi√≥n b√°sica.

Ahora, antes de terminar esta conferencia, quiero volver a la pregunta inicial que mencion√© que los

estudiantes me preguntan todo el tiempo cu√°l es esta idea de cu√°l es una precisi√≥n suficientemente buena.

Y como siempre, esto realmente depende del contexto de la situaci√≥n en la que est√° ejecutando su modelo.

No hay un n√∫mero √∫nico que pueda dar.

Decir algo as√≠ como una precisi√≥n del 90 por ciento es lo suficientemente bueno para cada situaci√≥n.

Ya vimos el problema con las clases desequilibradas, pero tampoco hay un comentario realmente bueno que pueda

dar sobre lo que es una precisi√≥n o recuerdo lo suficientemente bueno y tenemos que pensar en el contexto de la situaci√≥n.

Entonces, pensemos de nuevo en ese modelo que es diagn√≥stico para predecir la presencia de una enfermedad.

Entonces, si creamos un modelo para predecir la presencia de una enfermedad, lo primero en lo que queremos

pensar es si vamos a tener clases de equilibrio o clases desequilibradas.

Entonces, la presencia de la enfermedad est√° bien equilibrada en la poblaci√≥n general y, en la mayor√≠a de los casos, probablemente

no lo sea.

Probablemente no haya una enfermedad que vaya a suceder donde aproximadamente la mitad de la poblaci√≥n se ve afectada y

la mitad de la poblaci√≥n no se ve afectada.

Por lo tanto, ya podemos decir que tendremos una situaci√≥n de clase desequilibrada,

lo que significa que tendremos una compensaci√≥n de recuperaci√≥n de precisi√≥n, por lo que tambi√©n debemos tener en cuenta que a menudo estos modelos se usan como

pruebas de diagn√≥stico r√°pido antes de tener una prueba m√°s invasiva .

Por ejemplo, para los modelos de c√°ncer de pr√≥stata es muy com√∫n hacer una simple prueba de orina antes de hacerse

una biopsia porque es mucho m√°s f√°cil hacerse la prueba de orina que someterse a una biopsia completa

de la pr√≥stata.

Entonces, tambi√©n debemos considerar qu√© est√° en juego si se trata de c√°ncer de pr√≥stata.

Las apuestas son en realidad bastante altas.



------------------------------------
00:13:23,510 --> 00:13:25,500
Las apuestas son en realidad bastante altas.
------------------------------------



Entonces, como mencionamos, a menudo tenemos esa compensaci√≥n de precisi√≥n, lo que significa que esencialmente necesitamos decidir si el

modelo debe enfocarse en la fijaci√≥n de falsos positivos versus falsos negativos.

Entonces, a costa de disminuir los falsos negativos.

Lo m√°s probable es que aumentemos los falsos positivos y viceversa.

Y en el diagn√≥stico de la enfermedad, probablemente sea mejor tratar de minimizar la cantidad de falsos negativos

a costa de aumentar la cantidad de falsos positivos.

Entonces va en la direcci√≥n de estos falsos positivos.

¬øY por qu√© realmente queremos hacer eso?

Bueno, queremos asegurarnos de clasificar correctamente la mayor cantidad posible de casos de la enfermedad.

Entonces, a costa de aumentar los falsos positivos, hacemos todo lo posible para disminuir los falsos negativos.

En este contexto de diagn√≥stico de enfermedad y por qu√© realmente queremos hacer eso.

Bueno, queremos asegurarnos de que cualquier persona que realmente tenga alg√∫n tipo de presencia de esta enfermedad o en este caso

de este ejemplo, el c√°ncer de pr√≥stata.

Si estamos haciendo esa prueba de orina, queremos asegurarnos de que todas esas personas que tengan

la presencia de la enfermedad realmente pasen al siguiente paso.

Esa es quiz√°s una prueba m√°s invasiva como una biopsia.

Y recuerde que este tipo generalmente tiene un costo de aumentar esos falsos positivos.

Entonces, nuestros modelos no ser√°n perfectos y eventualmente suceder√° como alguien que

no tiene la enfermedad cuando hacemos nuestra simple prueba de orina o cualquiera que sea nuestro modelo

de aprendizaje autom√°tico va a predecir que hey, lo siento, pero tiene la presencia positiva de esta enfermedad y ser√°

un error porque ser√° un falso positivo.

Sin embargo, cuando reciban pruebas m√°s invasivas, se dar√°n cuenta de que lo siento, fue un falso positivo.

En realidad estas bien.

No tienes esta enfermedad y eso viene porque estamos tratando de minimizar los falsos negativos.

Lo que realmente no queremos cuando se trata de algo tan importante como tener c√°ncer o una enfermedad,

realmente no queremos rechazar a alguien que realmente tiene una enfermedad y decirles que no tienen la

enfermedad.

Por lo tanto, sea un falso negativo cuando se trata del diagn√≥stico de la enfermedad.

Eso es algo que realmente queremos minimizar.

Queremos minimizar esos falsos negativos para que no extra√±emos accidentalmente a alguien de inmediato con nuestra

herramienta de diagn√≥stico y les digamos que no tienen la enfermedad o que no tienen c√°ncer cuando realmente

la tienen.

Por lo tanto, podemos ver algo tan importante como que el diagn√≥stico de la enfermedad realmente minimiza esos

falsos negativos se vuelve realmente importante y casi todo esto es para decir que el aprendizaje autom√°tico no se realizar√° en

el vac√≠o.

Y estas m√©tricas de rendimiento no tienen alg√∫n tipo de verdad universal que sea cierta en todos los problemas,

pero el aprendizaje autom√°tico es que es un proceso realmente colaborativo en el que siempre debemos consultar

a expertos en el dominio.

Por ejemplo, en el caso del diagn√≥stico de la enfermedad, probablemente deber√≠amos hablar con los m√©dicos de

metal cola sobre cu√°les son nuestros est√°ndares aceptables para falsos negativos versus falsos positivos.

Pero como una presencia general de esta enfermedad en la poblaci√≥n general, etc., esto va a cambiar dependiendo del

contexto de su situaci√≥n, as√≠ que siempre tenga eso en cuenta.

No habr√° una respuesta f√°cil para m√≠ decir que es una precisi√≥n o una precisi√≥n

lo suficientemente buena porque todo depende del contexto y el dominio de la situaci√≥n.

Bien, acabamos de hablar sobre c√≥mo podemos evaluar los modelos de clasificaci√≥n.

Avancemos y pasemos a la pr√≥xima conferencia donde discutimos la evaluaci√≥n de las tareas de regresi√≥n.

Te ver√© all√°.


-----------------------------
00:16:35,990 --> 00:16:36,470
Te ver√© all√°.
-----------------------------










Bienvenidos de nuevo a todos.

Acabamos de discutir c√≥mo evaluar el rendimiento para los problemas de clasificaci√≥n.

Ahora comprendamos c√≥mo evaluar el rendimiento para las pruebas de regresi√≥n, de modo que podamos tomar un momento para analizar c√≥mo

evaluamos un modelo que est√° realizando alg√∫n tipo de tarea de regresi√≥n.

Y, en general, una regresi√≥n es la tarea cuando un modelo intenta predecir valores continuos.

A diferencia de las tareas de clasificaci√≥n en las que estamos tratando de predecir valores categ√≥ricos, es posible que haya o√≠do hablar

de algunas m√©tricas de evaluaci√≥n, como la precisi√≥n o el recuerdo o la precisi√≥n como acabamos de discutir.

Sin embargo, este tipo de m√©tricas no ser√°n √∫tiles para problemas de regresi√≥n.

Realmente no tiene sentido calcular la precisi√≥n o la recuperaci√≥n de una tarea de regresi√≥n ya que en

realidad est√° clasificando cosas.

En cambio, est√°s prediciendo un valor continuo.

Por lo tanto, necesitamos nuevas m√©tricas dise√±adas para esos valores continuos.

Por ejemplo, imagine que estamos intentando predecir el precio de una casa dadas sus caracter√≠sticas.

Esa ser√≠a una tarea de regresi√≥n o intentar predecir el pa√≠s.

Una casa est√° adentro.

Dadas sus caracter√≠sticas, ser√≠a una tarea de clasificaci√≥n.

Y nuevamente estamos enfocados ahora en c√≥mo evaluar esa tarea de regresi√≥n donde una

etiqueta es continua y no son categor√≠as separadas.

Por lo tanto, nuevamente para analizar las m√©tricas de evaluaci√≥n m√°s comunes para la regresi√≥n, que son el

error absoluto medio significa error al cuadrado y luego el error al cuadrado medio ra√≠z.

Comencemos con el m√°s simple, que es el error absoluto medio.

Y este es el m√°s f√°cil de entender, esencialmente todo lo que vas a hacer es comparar tus

predicciones y recordar que estamos comparando un valor continuo aqu√≠, ya que esta es una tarea de regresi√≥n,

vamos a comparar nuestras predicciones con las la etiqueta y verdadera, por ejemplo, compara la predicci√≥n real del precio

de la vivienda con el precio de la vivienda real y lo que hacemos es simplemente tomar la diferencia entre el precio

verdadero menos nuestro precio predicho, por eso tomamos el valor absoluto de eso y la raz√≥n por la que tomamos

el el valor absoluto se debe a que t√©cnicamente nuestras predicciones podr√≠an estar por encima o por

debajo del valor verdadero y dado que queremos promediar todas nuestras predicciones, tomamos el valor absoluto.

De acuerdo, ese es el error absoluto medio, esencialmente solo est√°s tomando las diferencias entre tu predicci√≥n

y la etiqueta verdadera, tomas el valor absoluto de eso y luego promedias

eso para todas tus predicciones.



--------------------------------
00:02:18,020 --> 00:02:20,190
eso para todas tus predicciones.
--------------------------------


Ahora el problema con el error absoluto medio es que no castigar√° los errores grandes.

As√≠ que aqu√≠ tenemos lo que se conoce como ans viene cuarteto y podemos ver aqu√≠ que tenemos una amplia variedad

de diferentes dispersiones de puntos de datos aqu√≠.

Sin embargo, la l√≠nea de mejor ajuste para todos estos es en realidad la misma.

Por eso queremos asegurarnos de que estamos al tanto de situaciones como esta.

Por ejemplo, echemos un vistazo a esta situaci√≥n espec√≠fica.

Tenemos un punto que es un valor at√≠pico enorme, por lo que queremos que nuestras m√©tricas de aire realmente los tengan en cuenta.

Entonces, lo que podemos hacer es usar un error cuadr√°tico medio.

Y esta es la media de los errores al cuadrado.

Entonces, lo que estamos haciendo aqu√≠ es tomar la diferencia entre el valor verdadero menos nuestro valor predicho y vamos a

ajustarlo al cuadrado.

Y como puede imaginar cuando realmente tomamos ese cuadrado, los errores m√°s grandes se notan m√°s que con el error absoluto

medio, lo que hace que el error al cuadrado sea m√°s popular porque realmente va a

castigar a su modelo por esas situaciones at√≠picas a las que no corresponde.

Y ya no necesitamos tomar el valor absoluto porque cualquier cosa al cuadrado termina siendo positiva.

Sin embargo, hay otro problema con el que nos encontramos con un error al cuadrado que cuadra la etiqueta

real menos su predicci√≥n.

En realidad tambi√©n cuadra las unidades mismas.

Entonces, por ejemplo, si est√° prediciendo el precio de una casa, nuestra m√©trica de error con error absoluto

medio ser√≠a en d√≥lares pero con error medio al cuadrado.

Terminamos obteniendo una m√©trica de error en unidades de d√≥lares al cuadrado que es dif√≠cil de interpretar, por lo que la forma

en que solucionamos esto es con la ra√≠z del error cuadr√°tico medio esencialmente al final, simplemente tomas

la ra√≠z cuadrada de tu error cuadr√°tico medio.

As√≠ que este es el m√°s popular porque castiga esos valores de error m√°s grandes y

al final del d√≠a tiene las mismas m√©tricas o las mismas unidades que y ahora.

La pregunta m√°s com√∫n que recibo de los estudiantes es: hey, este valor de ra√≠z significa error

al cuadrado que obtuve bien.

Y como siempre, el contexto lo es todo.

Imaginemos que estaba realizando alg√∫n tipo de modelo de maquinaria y obtuvo un error cuadr√°tico

medio de diez d√≥lares y me pregunta: ¬øEs este un error cuadr√°tico suficientemente bueno?

Bueno, realmente depende del contexto de la situaci√≥n.

Significa que un cuarto de diez d√≥lares ser√≠a fant√°stico si predices el precio de una casa, ya que diez

d√≥lares son muy peque√±os en comparaci√≥n con el precio promedio de una casa.

Pero si se supon√≠a que su modelo de aprendizaje autom√°tico predecir√≠a el precio de una barra de chocolate en funci√≥n de sus caracter√≠sticas

y su ruta significa que el trimestre era de diez d√≥lares.

En realidad, eso es realmente malo, por lo que debemos hacer es que debe comparar su m√©trica de error para la tarea

de regresi√≥n con el valor promedio de la etiqueta.

Ese es el valor medio en su conjunto de datos para tratar de obtener una intuici√≥n de su rendimiento general.

Y como siempre, el conocimiento de dominio tambi√©n juega un papel realmente importante aqu√≠.

Entonces, el aprendizaje autom√°tico nuevamente no se hace en el vac√≠o, generalmente se hace con un proceso colaborativo.

Entonces, si est√° prediciendo los precios de una casa y desea tener una idea de si su error de

rutina fue bueno o no, probablemente sea una buena idea comenzar a hablar con alguien con experiencia como

agente de bienes ra√≠ces.

Bien, ahora entendemos las diferentes m√©tricas que puede usar para evaluar el rendimiento de una tarea

de regresi√≥n.

Gracias y nos vemos en la pr√≥xima conferencia.














