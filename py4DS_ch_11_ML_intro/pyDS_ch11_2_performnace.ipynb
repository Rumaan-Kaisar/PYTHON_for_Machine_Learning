{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b89ab7",
   "metadata": {},
   "source": [
    "# ----  GPT  ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca8190",
   "metadata": {},
   "source": [
    "Below is a structured, textbook-style breakdown of the lecture on classification performance evaluation. First, the main topics are listed; then each is explained in clear English, preserving context, adding clarifications, and calling out any subtle misconceptions.\n",
    "\n",
    "---\n",
    "\n",
    "## üìë Topics Covered\n",
    "\n",
    "\n",
    "\n",
    "## 6. Key Metrics\n",
    "\n",
    "| Metric      | Formula                               | Interpretation                                  |\n",
    "|-------------|---------------------------------------|-------------------------------------------------|\n",
    "| **Accuracy**| (TP + TN) / (TP + TN + FP + FN)       | Overall fraction of correct predictions.        |\n",
    "| **Precision**| TP / (TP + FP)                       | Of all ‚Äúpositive‚Äù predictions, how many are correct? |\n",
    "| **Recall**  | TP / (TP + FN)                        | Of all true positives, how many did the model find? |\n",
    "| **F‚ÇÅ Score**| 2¬∑(Precision¬∑Recall)/(Precision+Recall)| Harmonic mean of precision and recall. Punishes extreme imbalance between them. |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Confusion Matrix  \n",
    "A 2√ó2 table summarizing TP, FP, FN, TN. It visually lays out prediction vs. reality:\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| **Actual Positive** | TP                 | FN                 |\n",
    "| **Actual Negative** | FP                 | TN                 |\n",
    "\n",
    "**Clarification:** In medical testing analogies, ‚Äúpositive‚Äù often means presence of disease.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Extending to Imbalanced Classes  \n",
    "When one class greatly outnumbers another, accuracy can be misleading. A model that always predicts the majority can have high accuracy yet fail entirely on the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Precision‚ÄìRecall Trade-off & F‚ÇÅ Harmonic Mean  \n",
    "- **Trade-off**: Raising the decision threshold may increase precision (fewer false alarms) but lower recall (more misses), and vice versa.  \n",
    "- **F‚ÇÅ Score**: The harmonic mean is used instead of arithmetic mean because it punishes extreme disparity.  \n",
    "  - If precision = 1.0 but recall = 0.0 (or vice versa), F‚ÇÅ = 0.0, reflecting the model‚Äôs failure in one dimension.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Contextual Metric Choice (e.g. Medical Diagnosis)  \n",
    "Metric importance depends on real-world cost:\n",
    "- **Minimizing FN** (false negatives) is critical when missing a positive (e.g. a disease) has high cost.  \n",
    "- **Minimizing FP** might matter more when false alarms incur expensive follow-up actions.\n",
    "\n",
    "**Clarification:** Always collaborate with domain experts (e.g. doctors) to set acceptable error trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. No ‚ÄúOne-Size-Fits-All‚Äù Metric  \n",
    "There is no universal ‚Äúgood‚Äù precision or recall threshold. Each application (spam filtering, medical screening, fraud detection) demands its own performance criteria, informed by domain stakes and class balance.\n",
    "\n",
    "---\n",
    "\n",
    "**Misconception Call-Out:**  \n",
    "> Thinking that a single train/test split with only accuracy suffices for model evaluation can mask over- or underfitting. Always consider validation splits and multiple metrics, especially in imbalanced scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "This completes the textbook-style clarification of classification evaluation metrics. Would you like a diagram of the confusion matrix or worked numeric examples next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf715ea",
   "metadata": {},
   "source": [
    "# ----  DS  ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c43cb1",
   "metadata": {},
   "source": [
    "## **Performance Evaluation for Classification Models**  \n",
    "Performance metrics quantify a model‚Äôs success and guide improvements. In classification, these metrics derive from comparing predicted labels to ground-truth labels on held-out (test or validation) data.\n",
    "\n",
    "\n",
    "* **Introduction to Model Evaluation**   \n",
    "   - Key Idea: After training, **Performance metrics** quantify how well the model generalizes to unseen data.  \n",
    "   - after training the model on training data, we'll use some sort of \"metric\" to see how well it perform on test/validation sets.  \n",
    "      \n",
    "\n",
    "### Review: Train vs. Test  \n",
    "- **Training set**: Data the model learns from (features X and known labels y).  \n",
    "- **Test set**: Separate data used only for final evaluation.  \n",
    "- **Validation set** (introduced later): Data used during development to tune hyperparameters without touching the final test.  \n",
    "\n",
    "**Clarification:** Never adjust your model's parameters based on test-set performance, or you risk overestimating real-world accuracy.\n",
    "\n",
    "\n",
    "### Prediction Outcomes: Correct vs. Incorrect  \n",
    "Every test example yields either a correct or incorrect prediction. In binary problems, collect counts of:\n",
    "\n",
    "- **True Positives (TP):** Model predicts ‚Äúpositive‚Äù and the true label is positive.  \n",
    "- **True Negatives (TN):** Model predicts ‚Äúnegative‚Äù and the true label is negative.  \n",
    "- **False Positives (FP):** Model predicts ‚Äúpositive‚Äù but the true label is negative.  \n",
    "- **False Negatives (FN):** Model predicts ‚Äúnegative‚Äù but the true label is positive.  \n",
    "\n",
    "Those four counts form the foundation of all classification metrics.\n",
    "\n",
    "* **Classification Metrics**  \n",
    "    following are the classification matrices we'll use:\n",
    "   - **Accuracy**:  \n",
    "     - Formula: \n",
    "        $$\n",
    "        \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "        $$\n",
    "     - **Limitation**: Misleading for **imbalanced datasets** (e.g., 99% \"dog\" images ‚Üí 99% accuracy by always predicting \"dog\").  \n",
    "   - **Recall (Sensitivity)**:  \n",
    "     - Measures: *\"How many actual positives were correctly predicted?\"*  \n",
    "     - Formula:  \n",
    "        $$\n",
    "        \\text{Recall (Sensitivity)} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "        $$      \n",
    "   - **Precision**:  \n",
    "     - Measures: *\"How many predicted positives are actual positives?\"*  \n",
    "     - Formula:  \n",
    "        $$\n",
    "        \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "        $$           \n",
    "   - **F1 Score**:  \n",
    "     - Harmonic mean of precision and recall. Penalizes extreme imbalances (e.g., high precision but low recall).  \n",
    "     - Formula:  \n",
    "        $$\n",
    "        \\text{F1\\_Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "        $$     \n",
    "\n",
    "\n",
    "## üéØ Reasoning behind these metrics and how they work   \n",
    "First, we need to understand the reasoning behind these metrics and how they are applied in practical scenarios.\n",
    "\n",
    "-   In any classification task, a model can only do one of two things: \n",
    "    * either make a correct prediction or \n",
    "    * an incorrect prediction  \n",
    "\n",
    "-   Every classification metric is built on this basic idea.\n",
    "\n",
    "- **In multi-class situations** (e.g. predicting A, B, C, or D):\n",
    "\n",
    "   * A prediction is **correct** if the predicted class matches the actual class.\n",
    "   * It‚Äôs **incorrect** if it predicts the wrong class.\n",
    "\n",
    "  **To simplify the explanation of classification metrics**, it's easier to start with **binary classification**:\n",
    "\n",
    "   * Only **two possible classes** (e.g., Class 0 and Class 1).\n",
    "   * This makes it clearer to understand concepts like\n",
    "     - true positives, \n",
    "     - false positives, \n",
    "     - true negatives, and \n",
    "     - false negatives.\n",
    "   * The same ideas behind these metrics can later be **extended to multi-class problems**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "### **Consider following Example**\n",
    "\n",
    "1. **Example:**\n",
    "   We want to predict whether a given image shows a dog or a cat.\n",
    "\n",
    "2. **Approach:**\n",
    "   This can be done using a **Convolutional Neural Network (CNN)**, which is a type of neural network designed for image data.\n",
    "\n",
    "3. **Supervised Learning:**\n",
    "   This is a **supervised learning problem** because we \"train or fit\" the model using images that already have known labels (either \"dog\" or \"cat\").\n",
    "   - This means we have images that have already been labeled as 'dog' or 'cat,' \n",
    "   - so we know the correct answer for each image.\n",
    "\n",
    "4. **Training Phase:**\n",
    "   In this phase:\n",
    "\n",
    "   * The model is shown many labeled images.\n",
    "   * It learns to find patterns that help it classify new images correctly.\n",
    "\n",
    "5. **Testing Phase:**\n",
    "   After training:\n",
    "\n",
    "   * The model is tested on new, unseen images (test data).\n",
    "   * It makes predictions on whether each image is a dog or a cat.\n",
    "\n",
    "6. **Evaluation:**\n",
    "\n",
    "   * The model's **predictions** are compared with the **true labels** (called **ground truth**) for these test images.\n",
    "     - So first get model's predictions for the test data (X)\n",
    "     - then compare them to the true labels (i.e. correct answers Y)\n",
    "   * This helps measure how well the model performs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Evaluation Process:**\n",
    "\n",
    "After training the model on the training data, we evaluate its performance using the **test dataset**.\n",
    "\n",
    "* Each test image is called **X_test** (the feature).\n",
    "* So the **image** itself is a feature, and this is from the **test set**\n",
    "* The corresponding correct label for that image is called **Y\\_test** (the ground truth).\n",
    "* We pass **X\\_test** to the model to get its prediction and then compare it to **Y\\_test** to see if the prediction is correct.\n",
    "* Say we have an image of a dog. We pass this image (as input features) into the already trained model, and the model makes a prediction.  \n",
    "\n",
    "  * **Correct prediction:** If the model \"predicts\" dog, and the \"correct label\" is also dog, the prediction is correct.  \n",
    "    i.e. $\\text{dog (prediction)} = \\text{dog (correct label)}$   \n",
    "\n",
    "  * **Incorrect prediction:** If it predicts cat instead, comparison with the correct label would be incorrect.  \n",
    "    i.e. $\\text{cat (prediction)} \\neq \\text{dog (correct label)}$  \n",
    "\n",
    "So in our casse, there are always two outcomes: **_correct_** or **_incorrect_**.\n",
    "  \n",
    "  \n",
    "* This process repeats for every image in **X\\_test**.\n",
    "\n",
    "* At the end, we count how many predictions were correct and how many were incorrect.\n",
    "\n",
    "* **Important point:** In real-world problems, not all correct or incorrect predictions have the same importance.\n",
    "\n",
    "* A single metric (like accuracy) often isn‚Äôt enough to describe model performance.\n",
    "\n",
    "* To properly evaluate a model, we look at **four key metrics** ‚Äî let‚Äôs revisit those and see how they‚Äôre calculated.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# üéà **Accuracy:**\n",
    "\n",
    "* We can organize predicted and actual values using a **confusion matrix** (we‚Äôll explain this later).\n",
    "\n",
    "### **Accuracy:**\n",
    "\n",
    "* Accuracy is one of the most common and easiest classification metrics to understand.\n",
    "* It measures how often the model makes correct predictions.\n",
    "\n",
    "  **Formula:**\n",
    "  Accuracy = (Number of correct predictions) √∑ (Total number of predictions)\n",
    "\n",
    "* In simple terms, it tells what **_percentage_** of predictions were correct.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$\n",
    "\n",
    "\n",
    "* **For example:**\n",
    "  If **X\\_test** has **100 images** and the model correctly predicts **80**, then:\n",
    "\n",
    "    $$\n",
    "    \\text{Accuracy} = \\frac{80}{100} = 0.8 = 80\\%\n",
    "    $$\n",
    "\n",
    "\n",
    "* **Accuracy** is most useful when classes are **well balanced**.\n",
    "* **Well balanced** means:\n",
    "  * The dataset has a similar number of images for each class.\n",
    "  * For example: about the same number of **cat** and **dog** images.\n",
    "  * The labels are evenly represented in the data.\n",
    "\n",
    "* **Why One Metric Is Not Enough**  \n",
    "  A single number (e.g. accuracy) may hide important behavior, especially with imbalanced classes.  \n",
    "  For example, a model that always predicts the majority class can achieve high accuracy but be useless for detecting the minority class.\n",
    "\n",
    " ### **Accuracy** isn't reliable when classes are **imbalanced.**\n",
    "\n",
    "* **What's an imbalanced class situation?**\n",
    "\n",
    "  * One class has many more examples than the other.\n",
    "  * Example: **99 dog images** and **1 cat image** in the test set.\n",
    "\n",
    "* **Thought experiment:**\n",
    "  * If we use a test set of 99 dogs and 1 cat images and\n",
    "  * If a model always predicts **dog**, \n",
    "  * so it would it be correct **99 times out of 100** on this particular test set\n",
    "  * This gives **99% accuracy**, even though the model completely ignores the **cat class**.\n",
    "\n",
    "* **Key point:**\n",
    "\n",
    "  * In imbalanced situations, accuracy can be misleading.\n",
    "  * It looks high but doesn‚Äôt reflect real performance on the minority class.\n",
    "\n",
    "* **When to use accuracy:**\n",
    "\n",
    "  * Works well if classes are balanced.\n",
    "  * Problematic if one class dominates.\n",
    "\n",
    "* That's why other metrics (like **precision**, **recall**, **F1 score**) are important when dealing with imbalanced data.\n",
    "\n",
    "---\n",
    "\n",
    "# üéà **Recall, Precision and F1 Score**\n",
    "\n",
    "* These metrics help evaluate model performance, especially with **imbalanced classes**.\n",
    "\n",
    "\n",
    "#### ‚úÖ **Recall**\n",
    "\n",
    "**Definition:**\n",
    "The proportion of **actual positive cases in the dataset** that the model correctly predicts as positive.  \n",
    "- Measures the model‚Äôs ability to **find all relevant cases** (i.e. how many actual positives it correctly identifies).  \n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "* **Numerator:** Correct positive predictions (**True Positives**).\n",
    "* **Denominator:** Total actual positives = True Positives + False Negatives.\n",
    "\n",
    "So we can write:  \n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{Correct positive predictions by model}}{\\text{All actual positives in dataset}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### ‚úÖ **Precision**\n",
    "\n",
    "**Definition:**\n",
    "The proportion of **positive predictions made by the model** that are actually correct.  \n",
    "- Measures how many of the positive predictions made by the model are actually correct.  \n",
    "- It measures the ability of a classification model to identify only relevant data points.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "* **Numerator:** Correct positive predictions (**True Positives**).\n",
    "* **Denominator:** All positive predictions made by the model = True Positives + False Positives.\n",
    "\n",
    "In other words:  \n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{Correct positive predictions by model}}{\\text{All positive predictions made by model, including incorrect ones)}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Trade-off Between Recall and Precision**\n",
    "\n",
    "* **Recall** focuses on finding **all relevant instances**.\n",
    "* **Precision** focuses on ensuring **what the model predicts as positive is actually positive**.\n",
    "* Often, improving one reduces the other ‚Äî a **trade-off**.\n",
    "* The **F1 Score** combines both into a single value, balancing precision and recall.  \n",
    "<br>  \n",
    "<br>\n",
    "  \n",
    "| Metric        | Clear Meaning                                                                                                | Formula            |\n",
    "| ------------- | ------------------------------------------------------------------------------------------------------------ | ------------------ |\n",
    "| **Recall**    | $$\\frac{\\text{Correct positive predictions by model}}{\\text{All actual positives in dataset}}$$                                  | $$\\frac{TP}{TP+FN}$$ |\n",
    "| **Precision** | $$\\frac{\\text{Correct positive predictions by model}}{\\text{All positive predictions made by model, including incorrect ones)}}$$ | $$\\frac{TP}{TP+FP}$$ |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **F1 Score**\n",
    "\n",
    "* Used when we want a **balanced combination of precision and recall**.\n",
    "\n",
    "* It‚Äôs not a simple average ‚Äî it uses the **harmonic mean**, which gives more weight to lower values.\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "\n",
    "* This ensures both precision and recall are fairly considered.\n",
    "  A low value in either will pull the F1 score down, encouraging a balanced performance.\n",
    "\n",
    "\n",
    "**Why use the harmonic mean for F1 score?**\n",
    "\n",
    "* A **simple average** would treat both values equally, even if one is very poor.\n",
    "* The **harmonic mean** punishes extreme differences between precision and recall.\n",
    "* **Example:**\n",
    "  If a model has:\n",
    "\n",
    "  * Precision = 1.0 (perfect)\n",
    "  * Recall = 0 (worst)  \n",
    "    A simple average = 0.5  \n",
    "    But the **F1 score = 0**, because:\n",
    "\n",
    "  $$\n",
    "  F1 = \\frac{2 \\times 1.0 \\times 0}{1.0 + 0} = 0\n",
    "  $$\n",
    "* This makes F1 a fairer way to combine precision and recall, especially when they differ a lot ‚Äî ensuring one bad value drags the score down.\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "# üéà **Confusion Matrix**\n",
    "\n",
    "* A **confusion matrix** shows how many predictions were **correct** or **incorrect** by comparing predicted and actual labels.\n",
    "\n",
    "---\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "\n",
    "|                    |                      | **Predicted Condition**        |                                 |\n",
    "| :----------------- | :------------------- | :----------------------------- | :------------------------------ |\n",
    "|                    | **Total Population** | **Predicted Positive**         | **Predicted Negative**          |\n",
    "| **True Condition** | **Actual Positive**  | True Positive (TP)             | False Negative (FN) *(Type II error)* |\n",
    "|                    | **Actual Negative**  | False Positive (FP) *(Type I error)* | True Negative (TN)              |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Key Ideas:**\n",
    "\n",
    "* **True Condition:**\n",
    "  The actual, correct label ‚Äî e.g. whether the image is *actually a dog* or *not a dog*, or in medical tests, *has a disease* or *doesn't have it*.\n",
    "\n",
    "* **Predicted Condition:**\n",
    "  What the model predicts ‚Äî positive or negative.\n",
    "\n",
    "* **Use Case Example:**\n",
    "  Think of this like a medical test:\n",
    "\n",
    "  * *Positive prediction* ‚Üí model says person has disease\n",
    "  * *Negative prediction* ‚Üí model says person doesn‚Äôt have the disease  \n",
    "    The confusion matrix tracks where the model was right and wrong.\n",
    "\n",
    "\n",
    "\n",
    "‚úÖ **Correct predictions:**  \n",
    "\n",
    "* **True Positive (TP):** The person *has the disease*, and the model correctly predicts positive.  \n",
    "* **True Negative (TN):** The person *does not have the disease*, and the model correctly predicts negative.  \n",
    "\n",
    "These are the **correct predictions**.  \n",
    "\n",
    "\n",
    "‚úÖ Then there are two types of **incorrect predictions**:  \n",
    "\n",
    "* **False Positive (FP):** The person *does not have the disease*, but the model incorrectly predicts positive.\n",
    "  - Also called a **Type I error**.\n",
    "\n",
    "* **False Negative (FN):** The person *has the disease*, but the model incorrectly predicts negative.\n",
    "  - Also called a **Type II error**.\n",
    "\n",
    "\n",
    "#### üö® **Which is worse?**\n",
    "\n",
    "In most **medical diagnosis or critical safety systems**, **Type II Error (False Negative)** is considered worse because:\n",
    "\n",
    "* It means failing to detect a real problem.\n",
    "* Consequences can be life-threatening or severe if not treated.\n",
    "\n",
    "**Example:**\n",
    "If a cancer test returns a false negative, the patient might not get necessary treatment in time.\n",
    "\n",
    "However, in some other contexts, **Type I Error** might be more costly or problematic (like false alarms in security systems or spam filters).\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [rev:16-May-2025]\n",
    "\n",
    "\n",
    "3. **Confusion Matrix**  \n",
    "   - A table comparing predicted vs. actual labels:  \n",
    "     - **True Positives (TP)**: Correctly predicted positives.  \n",
    "     - **True Negatives (TN)**: Correctly predicted negatives.  \n",
    "     - **False Positives (FP)**: Incorrectly predicted positives (*Type I error*).  \n",
    "     - **False Negatives (FN)**: Incorrectly predicted negatives (*Type II error*).  \n",
    "   - **Application**: Critical in fields like medical diagnosis (e.g., cancer screening).  \n",
    "\n",
    "4. **Trade-offs & Real-World Context**  \n",
    "   - **Precision-Recall Trade-off**:  \n",
    "     - *High recall* (minimize FNs) often increases FPs (e.g., in disease diagnosis, missing a case is worse than false alarms).  \n",
    "     - *High precision* (minimize FPs) may miss true cases (e.g., spam filtering).  \n",
    "   - **Domain-Specific Decisions**:  \n",
    "     - Example: In cancer testing, prioritize **low FNs** (avoid missing patients) even if it raises FPs (follow-up tests can clarify).  \n",
    "\n",
    "5. **Misconceptions Clarified**  \n",
    "   - **Accuracy is Not Always Reliable**:  \n",
    "     - The text initially highlights accuracy but later emphasizes its pitfalls in imbalanced datasets.  \n",
    "   - **\"One Metric Fits All\" Fallacy**:  \n",
    "     - No universal \"good\" metric‚Äîdepends on the problem (e.g., fraud detection vs. movie reviews).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Key Insights & Corrections:**  \n",
    "- **Binary vs. Multiclass**:  \n",
    "  - Metrics extend to multiclass problems (e.g., \"correct/incorrect\" per class), but binary examples simplify explanations.  \n",
    "- **F1 Score Nuance**:  \n",
    "  - The text correctly notes F1 is a **harmonic mean** (not arithmetic), which harshly penalizes low values in either precision or recall.  \n",
    "- **Context Matters**:  \n",
    "  - The lecture stresses consulting domain experts (e.g., doctors for medical models) to set acceptable FP/FN thresholds.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Pedagogical Approach:**  \n",
    "- **Simplification for Teaching**:  \n",
    "  - Uses binary classification (dog vs. cat) to introduce concepts but hints at scalability to multiclass.  \n",
    "- **Practical Warning**:  \n",
    "  - Warns against over-relying on test-set metrics without validation sets (echoing prior lecture‚Äôs train-validate-test split).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Final Summary:**  \n",
    "This text is a **lecture on evaluating classification models**, covering:  \n",
    "1. Core metrics (accuracy, precision, recall, F1).  \n",
    "2. **Confusion matrices** as a foundational tool.  \n",
    "3. The **criticality of context** (e.g., medical diagnosis vs. spam filtering).  \n",
    "4. **Trade-offs** between false positives/negatives and their real-world implications.  \n",
    "\n",
    "**Next Topic**: Performance evaluation for **regression tasks** (likely MSE, R-squared).  \n",
    "\n",
    "**Need deeper dives?** Ask about specific metrics or real-world examples! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4e887",
   "metadata": {},
   "source": [
    "# ----  DS  ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a19062",
   "metadata": {},
   "source": [
    "### **Analysis of the Text: Regression Error Metrics**  \n",
    "\n",
    "#### **1. Core Topics Identified:**  \n",
    "\n",
    "1. **Introduction to Regression Evaluation**  \n",
    "   - **Regression vs. Classification**:  \n",
    "     - Regression predicts **continuous values** (e.g., house prices).  \n",
    "     - Classification predicts **categorical values** (e.g., spam vs. legitimate emails).  \n",
    "   - **Key Difference**: Metrics like accuracy/precision/recall (used in classification) are irrelevant for regression.  \n",
    "\n",
    "2. **Regression Error Metrics**  \n",
    "   - **Mean Absolute Error (MAE)**:  \n",
    "     - Formula: `Average of |True Value ‚àí Predicted Value|`.  \n",
    "     - **Pros**: Easy to interpret (same units as the target variable, e.g., dollars for house prices).  \n",
    "     - **Cons**: Does not penalize large errors heavily (treats all errors equally).  \n",
    "   - **Mean Squared Error (MSE)**:  \n",
    "     - Formula: `Average of (True Value ‚àí Predicted Value)¬≤`.  \n",
    "     - **Pros**: Punishes larger errors more severely (useful for outlier-sensitive tasks).  \n",
    "     - **Cons**: Units are squared (e.g., dollars¬≤), making interpretation harder.  \n",
    "   - **Root Mean Squared Error (RMSE)**:  \n",
    "     - Formula: `‚àöMSE`.  \n",
    "     - **Pros**: Retains MSE‚Äôs outlier sensitivity but restores original units (e.g., dollars).  \n",
    "     - **Most popular** for regression tasks.  \n",
    "\n",
    "3. **Contextual Interpretation of Metrics**  \n",
    "   - **No Universal \"Good\" Value**:  \n",
    "     - Example: An RMSE of $10 is excellent for house price prediction but terrible for candy bar prices.  \n",
    "   - **Domain Knowledge is Critical**:  \n",
    "     - Compare error metrics to the **average target value** (e.g., RMSE of $10 vs. average house price of $300K).  \n",
    "     - Collaborate with domain experts (e.g., real estate agents for housing models).  \n",
    "\n",
    "4. **Visualizing Trade-offs**  \n",
    "   - **Anscombe‚Äôs Quartet Example**:  \n",
    "     - Four datasets with identical statistical properties (e.g., mean, variance) but vastly different distributions.  \n",
    "     - Highlights why **visualizing data** is as important as calculating metrics.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Key Clarifications & Corrections:**  \n",
    "- **Misconception**: \"MAE is always better because it‚Äôs simpler.\"  \n",
    "  - **Reality**: MAE is robust to outliers but may hide significant prediction flaws. MSE/RMSE are preferred when large errors are costly (e.g., medical dosing).  \n",
    "- **Units Matter**:  \n",
    "  - The text correctly notes that MSE‚Äôs squared units are unintuitive, but RMSE fixes this.  \n",
    "- **Error Metric Selection**:  \n",
    "  - Not explicitly stated: **Huber Loss** (a hybrid of MAE/MSE) is another option for balancing outlier sensitivity and interpretability.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Pedagogical Approach:**  \n",
    "- **Simplification**: Uses house price prediction as an intuitive example.  \n",
    "- **Real-World Analogy**: Contrasts RMSE applicability for housing (good) vs. candy bars (bad).  \n",
    "- **Warning Against Blind Metrics**: Emphasizes that error values must be compared to the dataset‚Äôs scale.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Final Summary:**  \n",
    "This lecture explains **regression evaluation metrics**:  \n",
    "1. **MAE**: Simple but ignores outlier severity.  \n",
    "2. **MSE**: Punishes large errors but hard to interpret.  \n",
    "3. **RMSE**: Best of both worlds (sensitive to outliers + interpretable units).  \n",
    "4. **Context is King**: No metric is universally \"good\"‚Äîalways compare to domain-specific benchmarks.  \n",
    "\n",
    "**Next Topic**: Likely model tuning (e.g., hyperparameter optimization) or advanced regression techniques.  \n",
    "\n",
    "**Need practical examples?** Ask about applying these metrics to specific datasets! üè°üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c85b79",
   "metadata": {},
   "source": [
    "|                    |                      | **Predicted Condition**        |                                 |\n",
    "| :----------------- | :------------------- | :----------------------------- | :------------------------------ |\n",
    "|                    | **Total Population** | **Predicted Positive**         | **Predicted Negative**          |\n",
    "| **True Condition** | **Actual Positive**  | True Positive (TP)             | False Negative (FN) *(Type II error)* |\n",
    "|                    | **Actual Negative**  | False Positive (FP) *(Type I error)* | True Negative (TN)              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9245a0b",
   "metadata": {},
   "source": [
    "Excellent ‚Äî let‚Äôs carefully write out the clean **formulas using confusion matrix terms (TP, FP, FN, TN)** and include the summation style for clarity where it applies.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Confusion Matrix Recap**\n",
    "\n",
    "|                     | **Predicted Positive** | **Predicted Negative** |\n",
    "| :------------------ | :--------------------- | :--------------------- |\n",
    "| **Actual Positive** | TP                     | FN                     |\n",
    "| **Actual Negative** | FP                     | TN                     |\n",
    "\n",
    "Where:\n",
    "\n",
    "* TP = True Positives\n",
    "* FP = False Positives\n",
    "* FN = False Negatives\n",
    "* TN = True Negatives\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Formulas (using summation style notation where suitable)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Accuracy**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Positive Predictive Value (PPV)** / **Precision**\n",
    "\n",
    "$$\n",
    "\\text{PPV} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **False Discovery Rate (FDR)**\n",
    "\n",
    "$$\n",
    "\\text{FDR} = \\frac{FP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **False Omission Rate (FOR)**\n",
    "\n",
    "$$\n",
    "\\text{FOR} = \\frac{FN}{FN + TN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ **Negative Predictive Value (NPV)**\n",
    "\n",
    "$$\n",
    "\\text{NPV} = \\frac{TN}{TN + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Prevalence**\n",
    "\n",
    "$$\n",
    "\\text{Prevalence} = \\frac{TP + FN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "(*proportion of actual positives in the dataset*)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **True Positive Rate (TPR)** / **Sensitivity / Recall / Probability of Detection**\n",
    "\n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **False Positive Rate (FPR)** / **Fall-out / Probability of False Alarm**\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ûï **Positive Likelihood Ratio (LR+)**\n",
    "\n",
    "$$\n",
    "\\text{LR+} = \\frac{\\text{TPR}}{\\text{FPR}} = \\frac{\\frac{TP}{TP + FN}}{\\frac{FP}{FP + TN}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ûñ **Negative Likelihood Ratio (LR‚àí)**\n",
    "\n",
    "$$\n",
    "\\text{LR-} = \\frac{\\text{FNR}}{\\text{TNR}} = \\frac{\\frac{FN}{TP + FN}}{\\frac{TN}{FP + TN}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* **FNR** (False Negative Rate) = $\\frac{FN}{TP + FN}$\n",
    "* **TNR** (True Negative Rate / Specificity) = $\\frac{TN}{FP + TN}$\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ All formulas expressed directly from the confusion matrix, neatly.\n",
    "Would you like me to wrap them into a table for even cleaner lookup too?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6355fa",
   "metadata": {},
   "source": [
    "|                    |                      | **Predicted Positive** | **Predicted Negative** | **Metric**                                          | **Formula**                                                                    |\n",
    "| :----------------- | :------------------- | :--------------------- | :--------------------- | :-------------------------------------------------- | :----------------------------------------------------------------------------- |\n",
    "|                    | **Total Population** |                        |                        | **Prevalence**                                      | $\\displaystyle \\frac{\\sum \\text{Condition Positive}}{\\text{Total Population}}$ |\n",
    "| **True Condition** | **Actual Positive**  | True Positive (TP)     | False Negative (FN)    | **True Positive Rate (TPR)**<br>Sensitivity, Recall | $\\displaystyle \\frac{\\sum TP}{\\sum \\text{Condition Positive}}$                 |\n",
    "|                    | **Actual Negative**  | False Positive (FP)    | True Negative (TN)     | **False Positive Rate (FPR)**<br>Fall-out           | $\\displaystyle \\frac{\\sum FP}{\\sum \\text{Condition Negative}}$                 |\n",
    "|                    |                      |                        |                        | **Accuracy**                                        | $\\displaystyle \\frac{\\sum TP + \\sum TN}{\\text{Total Population}}$              |\n",
    "|                    |                      |                        |                        | **Positive Predictive Value (PPV)**<br>Precision    | $\\displaystyle \\frac{\\sum TP}{\\sum \\text{Prediction Positive}}$                |\n",
    "|                    |                      |                        |                        | **False Discovery Rate (FDR)**                      | $\\displaystyle \\frac{\\sum FP}{\\sum \\text{Prediction Positive}}$                |\n",
    "|                    |                      |                        |                        | **False Omission Rate (FOR)**                       | $\\displaystyle \\frac{\\sum FN}{\\sum \\text{Prediction Negative}}$                |\n",
    "|                    |                      |                        |                        | **Negative Predictive Value (NPV)**                 | $\\displaystyle \\frac{\\sum TN}{\\sum \\text{Prediction Negative}}$                |\n",
    "|                    |                      |                        |                        | **Positive Likelihood Ratio (LR‚Å∫)**                 | $\\displaystyle \\frac{\\text{TPR}}{\\text{FPR}}$                                  |\n",
    "|                    |                      |                        |                        | **Negative Likelihood Ratio (LR‚Åª)**                 | $\\displaystyle \\frac{\\text{FNR}}{\\text{TNR}}$                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa24c6d",
   "metadata": {},
   "source": [
    "Ah ‚Äî thank you for clarifying! You meant you‚Äôd like me to **recreate the table as shown in the image** ‚Äî but written out in clean markdown or text here, with the exact formulas in LaTeX format where appropriate.\n",
    "Let‚Äôs do that properly:\n",
    "\n",
    "---\n",
    "\n",
    "## üìë Recreated Confusion Matrix Table with Exact Formulas\n",
    "\n",
    "| **True Condition / Predicted Condition** | **Prediction Positive**                                                              | **Prediction Negative**                    |\n",
    "| :--------------------------------------- | :----------------------------------------------------------------------------------- | :----------------------------------------- |\n",
    "| **Condition Positive**                   | **True Positive (TP)**<br><br>**Type II Error (FN)**: \\$\\text{False Negative (FN)}\\$ | **False Negative (FN)**<br>(Type II error) |\n",
    "| **Condition Negative**                   | **False Positive (FP)**<br>(Type I error)                                            | **True Negative (TN)**                     |\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Metric Formulas\n",
    "\n",
    "| **Metric**                                        | **Formula**                                                        |\n",
    "| :------------------------------------------------ | :----------------------------------------------------------------- |\n",
    "| **Prevalence**                                    | \\$\\frac{\\sum \\text{Condition Positive}}{\\text{Total Population}}\\$ |\n",
    "| **True Positive Rate (TPR)**, Sensitivity, Recall | \\$\\frac{\\sum TP}{\\sum \\text{Condition Positive}}\\$                 |\n",
    "| **False Positive Rate (FPR)**, Fall-out           | \\$\\frac{\\sum FP}{\\sum \\text{Condition Negative}}\\$                 |\n",
    "| **Accuracy**                                      | \\$\\frac{\\sum TP + \\sum TN}{\\text{Total Population}}\\$              |\n",
    "| **Positive Predictive Value (PPV), Precision**    | \\$\\frac{\\sum TP}{\\sum \\text{Prediction Positive}}\\$                |\n",
    "| **False Discovery Rate (FDR)**                    | \\$\\frac{\\sum FP}{\\sum \\text{Prediction Positive}}\\$                |\n",
    "| **False Omission Rate (FOR)**                     | \\$\\frac{\\sum FN}{\\sum \\text{Prediction Negative}}\\$                |\n",
    "| **Negative Predictive Value (NPV)**               | \\$\\frac{\\sum TN}{\\sum \\text{Prediction Negative}}\\$                |\n",
    "| **Positive Likelihood Ratio (LR+)\\$^{\\*}\\$**      | \\$\\frac{TPR}{FPR}\\$                                                |\n",
    "| **Negative Likelihood Ratio (LR‚àí)\\$^{\\*}\\$**      | \\$\\frac{FNR}{TNR}\\$                                                |\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* \\$\\sum\\$ indicates the sum/count of each respective type of case (TP, FP, etc.)\n",
    "* \\$FNR\\$ (False Negative Rate) is \\$1 - TPR\\$\n",
    "* \\$TNR\\$ (True Negative Rate) is \\$1 - FPR\\$\n",
    "\n",
    "---\n",
    "\n",
    "Would you like this styled as an actual markdown table too? I can drop that for you as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562943fa",
   "metadata": {},
   "source": [
    "$\\text{Prevalence} = \\frac{\\sum \\text{TP} + \\sum \\text{FN}}{\\text{Total Population}}$\n",
    "$$\n",
    "$\\text{True Positive Rate (TPR)} = \\frac{\\sum \\text{TP}}{\\sum \\text{Condition Positive}}$\n",
    "$$\n",
    "$\\text{False Positive Rate (FPR)} = \\frac{\\sum \\text{FP}}{\\sum \\text{Condition Negative}}$\n",
    "$$\n",
    "$\\text{Accuracy} = \\frac{\\sum \\text{TP} + \\sum \\text{TN}}{\\text{Total Population}}$\n",
    "$$\n",
    "$\\text{Positive Predictive Value (PPV), Precision} = \\frac{\\sum \\text{TP}}{\\sum \\text{Prediction Positive}}$\n",
    "$$\n",
    "$\\text{False Discovery Rate (FDR)} = \\frac{\\sum \\text{FP}}{\\sum \\text{Prediction Positive}}$\n",
    "$$\n",
    "$\\text{False Omission Rate (FOR)} = \\frac{\\sum \\text{FN}}{\\sum \\text{Prediction Negative}}$\n",
    "$$\n",
    "$\\text{Negative Predictive Value (NPV)} = \\frac{\\sum \\text{TN}}{\\sum \\text{Prediction Negative}}$\n",
    "$$\n",
    "$\\text{Positive Likelihood Ratio (LR+)} = \\frac{\\text{TPR}}{\\text{FPR}}$\n",
    "$$\n",
    "$\\text{Negative Likelihood Ratio (LR-)} = \\frac{\\text{FNR}}{\\text{TNR}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bbcfb",
   "metadata": {},
   "source": [
    "Awesome ‚Äî let‚Äôs unpack this step-by-step. The image you shared is a classic **confusion matrix summary for binary classification metrics**. I‚Äôll:\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 1Ô∏è‚É£ List and Find the Formulas:\n",
    "\n",
    "From the image, here are the formulas:\n",
    "\n",
    "* **Prevalence**\n",
    "* **True Positive Rate (TPR), Sensitivity, Recall**\n",
    "* **False Positive Rate (FPR)**\n",
    "* **Accuracy**\n",
    "* **Positive Predictive Value (PPV), Precision**\n",
    "* **False Discovery Rate (FDR)**\n",
    "* **False Omission Rate (FOR)**\n",
    "* **Negative Predictive Value (NPV)**\n",
    "* **Positive Likelihood Ratio (LR+)**\n",
    "* **Negative Likelihood Ratio (LR‚àí)**\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 2Ô∏è‚É£ Explanation of Each Formula:\n",
    "\n",
    "| **Metric**                                        | **Meaning**                                                                           |\n",
    "| :------------------------------------------------ | :------------------------------------------------------------------------------------ |\n",
    "| **Prevalence**                                    | How common the positive condition is in the total population.                         |\n",
    "| **True Positive Rate (TPR)**, Sensitivity, Recall | The proportion of actual positives correctly identified as positive.                  |\n",
    "| **False Positive Rate (FPR)**                     | The proportion of actual negatives incorrectly identified as positive.                |\n",
    "| **Accuracy**                                      | The proportion of all predictions (both positive and negative) that are correct.      |\n",
    "| **Positive Predictive Value (PPV)**, Precision    | The proportion of positive predictions that are correct.                              |\n",
    "| **False Discovery Rate (FDR)**                    | The proportion of positive predictions that are actually incorrect (false positives). |\n",
    "| **False Omission Rate (FOR)**                     | The proportion of negative predictions that are actually incorrect (false negatives). |\n",
    "| **Negative Predictive Value (NPV)**               | The proportion of negative predictions that are correct.                              |\n",
    "| **Positive Likelihood Ratio (LR+)**               | How much the odds of the condition increase when a test is positive.                  |\n",
    "| **Negative Likelihood Ratio (LR‚àí)**               | How much the odds of the condition decrease when a test is negative.                  |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 3Ô∏è‚É£ Formulas in LaTeX (TeX format)\n",
    "\n",
    "I‚Äôll write them cleanly in LaTeX:\n",
    "\n",
    "```latex\n",
    "\\text{Prevalence} = \\frac{\\sum \\text{TP} + \\sum \\text{FN}}{\\text{Total Population}}\n",
    "\n",
    "\\text{True Positive Rate (TPR)} = \\frac{\\sum \\text{TP}}{\\sum \\text{Condition Positive}}\n",
    "\n",
    "\\text{False Positive Rate (FPR)} = \\frac{\\sum \\text{FP}}{\\sum \\text{Condition Negative}}\n",
    "\n",
    "\\text{Accuracy} = \\frac{\\sum \\text{TP} + \\sum \\text{TN}}{\\text{Total Population}}\n",
    "\n",
    "\\text{Positive Predictive Value (PPV), Precision} = \\frac{\\sum \\text{TP}}{\\sum \\text{Prediction Positive}}\n",
    "\n",
    "\\text{False Discovery Rate (FDR)} = \\frac{\\sum \\text{FP}}{\\sum \\text{Prediction Positive}}\n",
    "\n",
    "\\text{False Omission Rate (FOR)} = \\frac{\\sum \\text{FN}}{\\sum \\text{Prediction Negative}}\n",
    "\n",
    "\\text{Negative Predictive Value (NPV)} = \\frac{\\sum \\text{TN}}{\\sum \\text{Prediction Negative}}\n",
    "\n",
    "\\text{Positive Likelihood Ratio (LR+)} = \\frac{\\text{TPR}}{\\text{FPR}}\n",
    "\n",
    "\\text{Negative Likelihood Ratio (LR-)} = \\frac{\\text{FNR}}{\\text{TNR}}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 4Ô∏è‚É£ Make the Table in `.ipynb` (Jupyter Notebook) Format\n",
    "\n",
    "I‚Äôll generate the `.ipynb` content (in JSON) that recreates this table using `pandas.DataFrame` and display the formulas using Markdown/LaTeX rendering inside the notebook.\n",
    "\n",
    "Here‚Äôs the notebook content:\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ `confusion_matrix_metrics.ipynb` content:\n",
    "\n",
    "```json\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üìä Confusion Matrix Metrics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = {\\n\",\n",
    "    \"    'Metric': [\\n\",\n",
    "    \"        'Prevalence', 'True Positive Rate (TPR)', 'False Positive Rate (FPR)', 'Accuracy',\\n\",\n",
    "    \"        'Positive Predictive Value (PPV)', 'False Discovery Rate (FDR)', 'False Omission Rate (FOR)',\\n\",\n",
    "    \"        'Negative Predictive Value (NPV)', 'Positive Likelihood Ratio (LR+)', 'Negative Likelihood Ratio (LR-)'\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Formula': [\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma TP + \\\\\\\\Sigma FN}{\\\\\\\\text{Total Population}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma TP}{\\\\\\\\Sigma \\\\\\\\text{Condition Positive}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma FP}{\\\\\\\\Sigma \\\\\\\\text{Condition Negative}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma TP + \\\\\\\\Sigma TN}{\\\\\\\\text{Total Population}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma TP}{\\\\\\\\Sigma \\\\\\\\text{Prediction Positive}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma FP}{\\\\\\\\Sigma \\\\\\\\text{Prediction Positive}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma FN}{\\\\\\\\Sigma \\\\\\\\text{Prediction Negative}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{\\\\\\\\Sigma TN}{\\\\\\\\Sigma \\\\\\\\text{Prediction Negative}}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{TPR}{FPR}$',\\n\",\n",
    "    \"        r'$\\\\\\\\frac{FNR}{TNR}$'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"df = pd.DataFrame(data)\\n\",\n",
    "    \"df\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"To render these formulas as LaTeX, display this table in a Jupyter environment that supports LaTeX rendering.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.x\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Would you like me to generate this `.ipynb` file for direct download too? I can prep it for you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
